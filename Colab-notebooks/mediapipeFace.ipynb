{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mediapipeFace.py.ipynb","provenance":[],"collapsed_sections":["cN9WrLY4vDcM","eb3bHV5byItv"],"authorship_tag":"ABX9TyPQriDbot/rTXOO/2PYko4p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[""],"metadata":{"id":"bp8Ytj6XT0f7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mount Drive"],"metadata":{"id":"myYJB1mOsvJK"}},{"cell_type":"code","source":["#mount drive\n","%cd ..\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive\n","\n","# list the contents of /mydrive\n","!ls /mydrive\n","\n","#Navigate to /mydrive/mediapipe-main\n","%cd /mydrive/mediapipe-main\n","\n","!pip install mediapipe\n","!pip install kaleido\n","!pip install PyQt5\n","!pip install IPython"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUcYqnjNT0dY","executionInfo":{"status":"ok","timestamp":1650573850010,"user_tz":240,"elapsed":15173,"user":{"displayName":"Seth Howells","userId":"08049334453680049674"}},"outputId":"5b918417-1d04-4ce4-d8c8-9ff871931ce8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","ln: failed to create symbolic link '/mydrive/My Drive': File exists\n","'2022-04-06 21-23.pdf'\t\t    mediapipe-main\n"," ColabNB\t\t\t   'My Drive'\n","'Colab Notebooks'\t\t    Other\n","'Copy of Tumor_CNN_denoise.ipynb'   Tumor2\n"," Data-Viz\t\t\t   'Untitled document.gdoc'\n"," DistCompSys\t\t\t    videoplayback.mp4\n"," GSVPanoDepth.js-master\t\t    videos\n"," Howells_bslevels.gsheet\t    yolov4\n"," images\n","/content/gdrive/My Drive/mediapipe-main\n","Requirement already satisfied: mediapipe in /usr/local/lib/python3.7/dist-packages (0.8.9.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.8)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n","Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (0.2.1)\n","Requirement already satisfied: PyQt5 in /usr/local/lib/python3.7/dist-packages (5.15.6)\n","Requirement already satisfied: PyQt5-Qt5>=5.15.2 in /usr/local/lib/python3.7/dist-packages (from PyQt5) (5.15.2)\n","Requirement already satisfied: PyQt5-sip<13,>=12.8 in /usr/local/lib/python3.7/dist-packages (from PyQt5) (12.10.1)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (5.5.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython) (57.4.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython) (4.8.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython) (5.1.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython) (0.8.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython) (1.0.18)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython) (0.7.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython) (0.7.0)\n"]}]},{"cell_type":"markdown","source":["# Head Pose Estimation"],"metadata":{"id":"xefYvHldXBth"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"yk-VW8qKsdxB"}},{"cell_type":"code","source":["from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from google.colab import files\n","from google.colab.patches import cv2_imshow\n","from base64 import b64decode, b64encode\n","import PIL\n","import io\n","import glob\n","import os\n","import datetime\n","import imutils\n","import time\n","from time import sleep\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import mediapipe as mp"],"metadata":{"id":"lJXtWN21dACo","executionInfo":{"status":"ok","timestamp":1650573850382,"user_tz":240,"elapsed":382,"user":{"displayName":"Seth Howells","userId":"08049334453680049674"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Java Helper Functions for Real-Time webcam"],"metadata":{"id":"1Z6T_QcpsksW"}},{"cell_type":"code","source":["#\n","# based on: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi\n","#\n","\n","def init_camera():\n","  \"\"\"Create objects and functions in HTML/JavaScript to access local web camera\"\"\"\n","\n","  js = Javascript('''\n","\n","    // global variables to use in both functions\n","    var div = null;\n","    var video = null;   // <video> to display stream from local webcam\n","    var stream = null;  // stream from local webcam\n","    var canvas = null;  // <canvas> for single frame from <video> and convert frame to JPG\n","    var img = null;     // <img> to display JPG after processing with `cv2`\n","\n","    async function initCamera() {\n","      // place for video (and eventually buttons)\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      // <video> to display video\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      div.appendChild(video);\n","\n","      // get webcam stream and assing to <video>\n","      stream = await navigator.mediaDevices.getUserMedia({video: true});\n","      video.srcObject = stream;\n","\n","      // start playing stream from webcam in <video>\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // <canvas> for frame from <video>\n","      canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      //div.appendChild(input_canvas); // there is no need to display to get image (but you can display it for test)\n","\n","      // <img> for image after processing with `cv2`\n","      img = document.createElement('img');\n","      img.width = video.videoWidth;\n","      img.height = video.videoHeight;\n","      div.appendChild(img);\n","    }\n","\n","    async function takeImage(quality) {\n","      // draw frame from <video> on <canvas>\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      // stop webcam stream\n","      //stream.getVideoTracks()[0].stop();\n","\n","      // get data from <canvas> as JPG image decoded base64 and with header \"data:image/jpg;base64,\"\n","      return canvas.toDataURL('image/jpeg', quality);\n","      //return canvas.toDataURL('image/png', quality);\n","    }\n","\n","    async function showImage(image) {\n","      // it needs string \"data:image/jpg;base64,JPG-DATA-ENCODED-BASE64\"\n","      // it will replace previous image in `<img src=\"\">`\n","      img.src = image;\n","      // TODO: create <img> if doesn't exists, \n","      // TODO: use `id` to use different `<img>` for different image - like `name` in `cv2.imshow(name, image)`\n","    }\n","\n","  ''')\n","\n","  display(js)\n","  eval_js('initCamera()')\n","\n","def take_frame(quality=0.1):\n","  \"\"\"Get frame from web camera\"\"\"\n","\n","  data = eval_js('takeImage({})'.format(quality))  # run JavaScript code to get image (JPG as string base64) from <canvas>\n","\n","  header, data = data.split(',')  # split header (\"data:image/jpg;base64,\") and base64 data (JPG)\n","  data = b64decode(data)  # decode base64\n","  data = np.frombuffer(data, dtype=np.uint8)  # create numpy array with JPG data\n","\n","  img = cv2.imdecode(data, cv2.IMREAD_UNCHANGED)  # uncompress JPG data to array of pixels\n","\n","  return img\n","\n","def show_frame(img, quality=0.1):\n","  \"\"\"Put frame as <img src=\"data:image/jpg;base64,....\"> \"\"\"\n","\n","  ret, data = cv2.imencode('.jpg', img)  # compress array of pixels to JPG data\n","\n","  data = b64encode(data)  # encode base64\n","  data = data.decode()  # convert bytes to string\n","  data = 'data:image/jpg;base64,' + data  # join header (\"data:image/jpg;base64,\") and base64 data (JPG)\n","\n","  eval_js('showImage(\"{}\")'.format(data))  # run JavaScript code to put image (JPG as string base64) in <img>\n","                                           # argument in `showImage` needs `\" \"` "],"metadata":{"id":"tn0Viro3EB1H","executionInfo":{"status":"ok","timestamp":1650573850383,"user_tz":240,"elapsed":4,"user":{"displayName":"Seth Howells","userId":"08049334453680049674"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Head Pose detection"],"metadata":{"id":"i9QgZvDVstMr"}},{"cell_type":"code","source":["mp_face_mesh = mp.solutions.face_mesh\n","face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n","mp_drawing_styles = mp.solutions.drawing_styles\n","mp_drawing = mp.solutions.drawing_utils\n","drawing_spec = mp_drawing.DrawingSpec(color=(0,255,0),thickness=1, circle_radius=1) # green\n","\n","# start streaming video from webcam\n","init_camera()\n","label_html = 'Capturing...' # label for video\n","bbox = ''                   # initialze bounding box to empty\n","\n","# Define the codec and create VideoWriter object\n","fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","#out = cv2.VideoWriter('faceDetect_direction4.mp4',fourcc, 3.0, (640,480))\n","\n","while True:\n","  try:\n","    frame = take_frame()\n","\n","    start = time.time()\n","\n","    # Flip the image horizontally for a later selfie-view display\n","    # Also convert the color space from BGR to RGB\n","    image = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n","\n","    # To improve performance\n","    image.flags.writeable = False\n","    \n","    # Get the result\n","    results = face_mesh.process(image)\n","    \n","    # To improve performance\n","    image.flags.writeable = True\n","    \n","    # Convert the color space from RGB to BGR\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","    img_h, img_w, img_c = image.shape\n","    face_3d = []\n","    face_2d = []\n","\n","    if results.multi_face_landmarks:\n","        for face_landmarks in results.multi_face_landmarks:\n","            for idx, lm in enumerate(face_landmarks.landmark):\n","                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n","                    if idx == 1:\n","                        nose_2d = (lm.x * img_w, lm.y * img_h)\n","                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n","                    \n","                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n","                    # Get the 2D Coordinates\n","                    face_2d.append([x, y])\n","                    # Get the 3D Coordinates\n","                    face_3d.append([x, y, lm.z])       \n","            \n","            # Convert it to the NumPy array\n","            face_2d = np.array(face_2d, dtype=np.float64)\n","            # Convert it to the NumPy array\n","            face_3d = np.array(face_3d, dtype=np.float64)\n","\n","            # The camera matrix\n","            focal_length = 1 * img_w\n","            cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n","                                    [0, focal_length, img_w / 2],\n","                                    [0, 0, 1]])\n","\n","            # The distortion parameters\n","            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n","            # Solve PnP\n","            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n","            # Get rotational matrix\n","            rmat, jac = cv2.Rodrigues(rot_vec)\n","            # Get angles\n","            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n","\n","            # Get the y rotation degree\n","            x = angles[0] * 360\n","            y = angles[1] * 360\n","            z = angles[2] * 360\n","          \n","            # See where the user's head tilting\n","            if y < -10:\n","                text = \"Looking Left\"\n","            elif y > 10:\n","                text = \"Looking Right\"\n","            elif x < -10:\n","                text = \"Looking Down\"\n","            elif x > 10:\n","                text = \"Looking Up\"\n","            else:\n","                text = \"Forward\"\n","\n","            # Display the nose direction\n","            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n","            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n","            p2 = (int(nose_2d[0] + y * 10) , int(nose_2d[1] - x * 10))\n","            cv2.line(image, p1, p2, (255, 0, 0), 3)\n","            \n","            # Add the text on the image\n","            cv2.putText(image, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n","            cv2.putText(image, \"x: \" + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","            cv2.putText(image, \"y: \" + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","            cv2.putText(image, \"z: \" + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","\n","\n","            end = time.time()\n","            totalTime = end - start\n","            fps = 1 / totalTime\n","            #print(\"FPS: \", fps)\n","\n","            cv2.putText(image, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n","            \n","            mp_drawing.draw_landmarks(\n","                image=image,\n","                landmark_list=face_landmarks,\n","                connections=mp_face_mesh.FACEMESH_TESSELATION,\n","                landmark_drawing_spec=drawing_spec,\n","                connection_drawing_spec=mp_drawing_styles\n","                .get_default_face_mesh_tesselation_style())\n"," \n","    \n","    #out.write(image)    \n","    show_frame(image)\n","  except Exception as err:\n","      print('Exception:', err)\n","\n","#out.release()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tu5K6micT0S6","executionInfo":{"status":"error","timestamp":1650574401669,"user_tz":240,"elapsed":41517,"user":{"displayName":"Seth Howells","userId":"08049334453680049674"}},"outputId":"ce8ea50e-5d7a-4531-b90a-d4f94e1076a4"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","\n","    // global variables to use in both functions\n","    var div = null;\n","    var video = null;   // <video> to display stream from local webcam\n","    var stream = null;  // stream from local webcam\n","    var canvas = null;  // <canvas> for single frame from <video> and convert frame to JPG\n","    var img = null;     // <img> to display JPG after processing with `cv2`\n","\n","    async function initCamera() {\n","      // place for video (and eventually buttons)\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      // <video> to display video\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      div.appendChild(video);\n","\n","      // get webcam stream and assing to <video>\n","      stream = await navigator.mediaDevices.getUserMedia({video: true});\n","      video.srcObject = stream;\n","\n","      // start playing stream from webcam in <video>\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // <canvas> for frame from <video>\n","      canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      //div.appendChild(input_canvas); // there is no need to display to get image (but you can display it for test)\n","\n","      // <img> for image after processing with `cv2`\n","      img = document.createElement('img');\n","      img.width = video.videoWidth;\n","      img.height = video.videoHeight;\n","      div.appendChild(img);\n","    }\n","\n","    async function takeImage(quality) {\n","      // draw frame from <video> on <canvas>\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      // stop webcam stream\n","      //stream.getVideoTracks()[0].stop();\n","\n","      // get data from <canvas> as JPG image decoded base64 and with header \"data:image/jpg;base64,\"\n","      return canvas.toDataURL('image/jpeg', quality);\n","      //return canvas.toDataURL('image/png', quality);\n","    }\n","\n","    async function showImage(image) {\n","      // it needs string \"data:image/jpg;base64,JPG-DATA-ENCODED-BASE64\"\n","      // it will replace previous image in `<img src=\"\">`\n","      img.src = image;\n","      // TODO: create <img> if doesn't exists, \n","      // TODO: use `id` to use different `<img>` for different image - like `name` in `cv2.imshow(name, image)`\n","    }\n","\n","  "]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-fdfce9ce0a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#out.write(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mshow_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exception:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-8029a41563fa>\u001b[0m in \u001b[0;36mshow_frame\u001b[0;34m(img, quality)\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data:image/jpg;base64,'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m  \u001b[0;31m# join header (\"data:image/jpg;base64,\") and base64 data (JPG)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showImage(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run JavaScript code to put image (JPG as string base64) in <img>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                                            \u001b[0;31m# argument in `showImage` needs `\" \"`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[""],"metadata":{"id":"0dbSz9FzeQuF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#    Video to JPG"],"metadata":{"id":"cN9WrLY4vDcM"}},{"cell_type":"code","source":["# Imports \n","import mediapipe as mp\n","import cv2\n","import datetime\n","import imutils\n","from google.colab import files\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","import glob\n","\n","# Get video file from directory\n","video = 'data/input/headPose/headPose.mp4'\n","cap = cv2.VideoCapture(video)\n","\n","count = 0\n","# Save all images to new folder for image processing\n","while True:\n","  success,image = cap.read()\n","  cv2.imwrite(\"data/input/headPose/headpose%d.jpg\" % count, image)     # save frame as JPEG file      \n","  count += 1\n","  if False:\n","    break"],"metadata":{"id":"6x_2R0L7VHTS","colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"status":"error","timestamp":1650539811835,"user_tz":240,"elapsed":153,"user":{"displayName":"Seth Howells","userId":"08049334453680049674"}},"outputId":"44323223-0a96-405b-a611-051cf42f5e5c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-727a6c794ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["# Video processing"],"metadata":{"id":"eb3bHV5byItv"}},{"cell_type":"code","source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from google.colab.patches import cv2_imshow\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import glob\n","\n","# Read in folder containing images, store as dictionary: filename, image\n","dict_img = {name: cv2.imread(name) for name in glob.glob(\"data/input/headPose/headpose*.jpg\")}\n","IMAGE_FILES = dict_img \n","\n","mp_face_mesh = mp.solutions.face_mesh\n","face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n","mp_drawing = mp.solutions.drawing_utils\n","drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n","\n","\n","# For static images:\n","with mp_face_mesh.FaceMesh(\n","    static_image_mode=True,\n","    max_num_faces=1,\n","    refine_landmarks=True,\n","    min_detection_confidence=0.5) as face_mesh:\n","  for idx, file in enumerate(IMAGE_FILES):\n","    image = cv2.imread(file)\n","    \n","    # Flip the image horizontally for a later selfie-view display\n","    # Also convert the color space from BGR to RGB\n","    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n","\n","    # To improve performance\n","    image.flags.writeable = False\n","\n","    # Get the result\n","    results = face_mesh.process(image)\n","\n","    # To improve performance\n","    image.flags.writeable = True\n","\n","    # Convert the color space from RGB to BGR\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","    img_h, img_w, img_c = image.shape\n","    face_3d = []\n","    face_2d = []\n","\n","    if results.multi_face_landmarks:\n","        for face_landmarks in results.multi_face_landmarks:\n","            for idx, lm in enumerate(face_landmarks.landmark):\n","                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n","                    if idx == 1:\n","                        nose_2d = (lm.x * img_w, lm.y * img_h)\n","                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n","\n","                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n","\n","                    # Get the 2D Coordinates\n","                    face_2d.append([x, y])\n","\n","                    # Get the 3D Coordinates\n","                    face_3d.append([x, y, lm.z])       \n","            \n","            # Convert it to the NumPy array\n","            face_2d = np.array(face_2d, dtype=np.float64)\n","\n","            # Convert it to the NumPy array\n","            face_3d = np.array(face_3d, dtype=np.float64)\n","\n","            # The camera matrix\n","            focal_length = 1 * img_w\n","\n","            cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n","                                    [0, focal_length, img_w / 2],\n","                                    [0, 0, 1]])\n","\n","            # The distortion parameters\n","            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n","\n","            # Solve PnP\n","            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n","\n","            # Get rotational matrix\n","            rmat, jac = cv2.Rodrigues(rot_vec)\n","\n","            # Get angles\n","            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n","\n","            # Get the y rotation degree\n","            x = angles[0] * 360\n","            y = angles[1] * 360\n","            z = angles[2] * 360\n","          \n","\n","            # See where the user's head tilting\n","            if y < -10:\n","                text = \"Looking Left\"\n","            elif y > 10:\n","                text = \"Looking Right\"\n","            elif x < -10:\n","                text = \"Looking Down\"\n","            elif x > 10:\n","                text = \"Looking Up\"\n","            else:\n","                text = \"Forward\"\n","\n","            # Display the nose direction\n","            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n","\n","            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n","            p2 = (int(nose_2d[0] + y * 10) , int(nose_2d[1] - x * 10))\n","            \n","            cv2.line(image, p1, p2, (255, 0, 0), 3)\n","\n","            # Add the text on the image\n","            cv2.putText(image, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n","            cv2.putText(image, \"x: \" + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","            cv2.putText(image, \"y: \" + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","            cv2.putText(image, \"z: \" + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","\n","            cv2.putText(image, f'FPS:', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n","            #annotated = image.copy()\n","            mp_drawing.draw_landmarks(\n","                        image=image,\n","                        landmark_list=face_landmarks,\n","                        connections=mp_face_mesh.FACEMESH_TESSELATION,\n","                        landmark_drawing_spec=drawing_spec,\n","                        connection_drawing_spec=drawing_spec)\n","            cv2.imwrite('data/output/headPose/frame' + str(idx) + '.png', image)\n","            cv2_imshow(image)\n","            # Plot pose world landmarks.\n","            #mp_drawing.plot_landmarks(results.multi_face_landmarks, mp_face_mesh.FACEMESH_IRISES)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1SUFa-y1-_zwL1xBBve-qzNiaSuYoJuE7"},"id":"CyhFORKGyLkj","outputId":"7563db97-8ff6-40ec-e2c0-d4291386eb87"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}
